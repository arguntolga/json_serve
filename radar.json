{
  "title": "Internal Technology Radar",
  "quadrants": [
    "Languages & Frameworks",
    "Platforms",
    "Tools",
    "Techniques"
  ],
  "rings": [
    "Adopt",
    "Trial",
    "Assess",
    "Hold"
  ],
  "items": [
    {
      "name": "Continuous compliance",
      "quadrant": "Techniques",
      "ring": "Adopt",
      "isNew": false,
      "description": "<p><strong>Continuous compliance</strong> is the practice of ensuring that software development processes and technologies meet regulatory and security standards on an ongoing basis through automation. Manual compliance checks can slow development and introduce human error, whereas automated checks and audits provide faster feedback, clearer evidence and simplified reporting. </p>\r\n<p>By integrating <a href=\"/radar/techniques/security-policy-as-code\">policy-as-code</a> tools such as <a href=\"/radar/tools/open-policy-agent-opa\">Open Policy Agent</a> and generating <a href=\"/radar/techniques/software-bill-of-materials\">SBOMs</a> within CD pipelines — aligned with <a href=\"/radar/techniques/slsa\">SLSA</a> guidance — teams can detect and address compliance issues early. Codifying rules and best practices enforces standards consistently across teams without creating bottlenecks. <a href=\"/radar/languages/open-security-control-assessment-language-oscal\">OSCAL</a> also shows promise as a framework for automating compliance at scale.</p>\r\n<p>Practices and tooling for continuous compliance are now mature enough that it should be treated as a sensible default, which is why we’ve moved our recommendation to Adopt. The increasing use of AI in coding — and the accompanying risk of <a href=\"/radar/techniques/complacency-with-ai-generated-code\">complacency with AI-generated code</a> — makes embedding compliance into the development process more critical than ever.</p>"
    },
    {
      "name": "Curated shared instructions for software teams",
      "quadrant": "Techniques",
      "ring": "Adopt",
      "isNew": false,
      "description": "<p>For teams actively using AI in software delivery, the next step is moving beyond individual prompting toward <strong>curated instructions for software teams</strong>. This practice helps you apply AI effectively across all delivery tasks — not just coding — by sharing proven, high-quality instructions. The most straightforward way to implement this is by committing instruction files, such as an <a href=\"/radar/techniques/agents-md\">AGENTS.md</a>, directly to your project repository. Most AI-coding tools — including <a href=\"/radar/tools/cursor\">Cursor</a>, <a href=\"/radar/tools/windsurf\">Windsurf</a> and <a href=\"/radar/tools/claude-code\">Claude Code</a> — support sharing instructions through custom slash commands or workflows. For noncoding tasks, you can set up organization-wide prompt libraries ready to use. This systematic approach allows for continuous improvement: As soon as a prompt is refined, the entire team benefits, ensuring consistent access to the best AI instructions.</p>"
    },
    {
      "name": "Pre-commit hooks",
      "quadrant": "Techniques",
      "ring": "Adopt",
      "isNew": false,
      "description": "<p><strong><a href=\"https://git-scm.com/book/ms/v2/Customizing-Git-Git-Hooks\">Git hooks</a></strong> have been around for a long time, but we feel they’re still underused. The rise of AI-assisted and agentic coding has increased the risk of accidentally committing secrets or problematic code. While there are many mechanisms for code validation, such as <a href=\"https://martinfowler.com/articles/continuousIntegration.html\">Continuous Integration</a>, pre-commit hooks are a simple and effective safeguard that more teams should adopt. However, overloading hooks with slow-running checks can discourage developers from using them, so it's best to keep them minimal and focused on risks that are most effectively caught at this stage of the workflow, such as secret scanning.</p>"
    },
    {
      "name": "Using GenAI to understand legacy codebases",
      "quadrant": "Techniques",
      "ring": "Adopt",
      "isNew": false,
      "description": "<p>In recent months, we’ve seen clear evidence that <strong>using GenAI to understand legacy codebases</strong> can significantly accelerate comprehension of large and complex systems. Tools such as <a href=\"/radar/tools/cursor\">Cursor</a>, <a href=\"/radar/tools/claude-code\">Claude Code</a>, <a href=\"https://github.com/features/copilot\">Copilot</a>, <a href=\"https://windsurf.com/\">Windsurf</a>, <a href=\"https://aider.chat/\">Aider</a>, <a href=\"https://meetcody.ai/\">Cody</a>, <a href=\"https://swimm.io/\">Swimm</a>, <a href=\"https://getunblocked.com/\">Unblocked</a> and <a href=\"https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge\">PocketFlow-Tutorial-Codebase-Knowledge</a> help developers surface business rules, summarize logic and identify dependencies. Used alongside open frameworks and direct LLM prompting, they dramatically reduce the time needed to understand legacy codebases.</p>\r\n<p>Our experience across multiple clients shows that GenAI-assisted understanding of legacy systems is now a practical default rather than an experiment. Setup effort varies, particularly for advanced approaches such as <a href=\"/radar/techniques/graphrag\">GraphRAG</a>, and tends to scale with the size and complexity of the codebase being analyzed. Despite this, the impact on productivity gains are consistent and substantial. GenAI has become an essential part of how we explore and understand legacy systems.</p>"
    },
    {
      "name": "AGENTS.md",
      "quadrant": "Techniques",
      "ring": "Trial",
      "isNew": false,
      "description": "<p><strong><a href=\"https://agents.md/\">AGENTS.md</a></strong> is a common format for providing instructions to AI coding agents working on a project. Essentially a README file for agents, it has no required fields or formatting other than Markdown, relying on the ability of LLM-based coding agents to interpret human-written, human-readable guidance. Typical uses include tips on using tools in the coding environment, testing instructions and preferred practices for managing commits. While AI tools support various methods for <a href=\"/radar/techniques/context-engineering\">context engineering</a>, the value of AGENTS.md lies in creating a simple convention for a file that acts as a starting point.</p>"
    },
    {
      "name": "AI for code migrations",
      "quadrant": "Techniques",
      "ring": "Trial",
      "isNew": false,
      "description": "<p>Code migrations take many forms — from language rewrites to dependency or framework upgrades — and are rarely trivial, often requiring months of manual effort. One of our teams, when <a href=\"https://www.thoughtworks.com/insights/blog/generative-ai/automating-framework-upgrades-can-combination-of-AI-and-traditional-tooling-help\">upgrading their .NET framework version</a>, experimented with using AI to shorten the process. In the past, we blipped <a href=\"/radar/tools/openrewrite\">OpenRewrite</a>, a deterministic, rule-based refactoring tool. Using AI alone for such upgrades has often proven costly and prone to meandering conversations. Instead, the team combined traditional upgrade pipelines with agentic coding assistants to manage complex transitions. Rather than delegating a full upgrade, they broke the process into smaller, verifiable steps: analyzing compilation errors, generating migration diffs and validating tests iteratively. This hybrid approach positions AI coding agents as pragmatic collaborators in software maintenance. Industry examples, such as <a href=\"https://www.theregister.com/2025/01/16/google_ai_code_migration/\">Google’s large-scale int32-to-int64 migration</a>, reflect a similar trend. While our results are mixed in measurable time savings, the potential to reduce developer toil is clear and worth continued exploration.</p>"
    },
    {
      "name": "Delta Lake Liquid Clustering",
      "quadrant": "Techniques",
      "ring": "Trial",
      "isNew": false,
      "description": "<p><strong><a href=\"https://delta.io/blog/liquid-clustering/\">Liquid Clustering</a></strong> is a technique for <a href=\"/radar/platforms/delta-lake\">Delta Lake</a> tables that serves as an alternative to partitioning and Z-ordering. Historically, optimizing Delta tables for read performance required defining partition and Z-order keys at table creation based on anticipated query patterns. Modifying these keys later necessitates a full data rewrite. In contrast, Liquid Clustering employs a tree-based algorithm to cluster data based on designated keys, which can be incrementally changed without rewriting all data. This provides greater flexibility to support diverse query patterns, thereby reducing compute costs and enhancing read performance. Furthermore, the Databricks Runtime for Delta Lake supports <a href=\"https://docs.databricks.com/aws/en/delta/clustering#automatic-liquid-clustering\">Automatic Liquid Clustering</a> by analyzing historical query workloads, identifying optimal columns, and clustering data accordingly. Both standalone Delta Lake and Databricks Runtime users can leverage Liquid Clustering to optimize read performance.</p>"
    }
  ]
}
